{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Session - Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Practical session is about unsupervised learning. We will use the dimensionality reduction and clustering techniques presented this morning to analyze toy examples, recognize faces and segment skin lesion images.\n",
    "\n",
    "You have two weeks (18/04) to update a small report (2 jupyter notebooks + theoretical questions) to the *site p√©dagogique* of IMA205 under the section *Reports-TP*. You can answer in French or English. The deadline is 23:59 of the 18th of April. I remind you that the report is mandatory and evaluated. \n",
    "\n",
    "**All reports uploaded after the deadline will not be evaluated, namely grade equal to 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook, you can play with the toy examples shown during the lecture. \n",
    "\n",
    "First let's load the functions we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy import linalg as LA\n",
    "from scipy.stats import ortho_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions are used to create the data and plot the results. Analyse them and try to understand what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scenario(scenario=3, n_samples0 = 100, n_samples1 = 30):\n",
    "\n",
    "    y = np.concatenate((np.zeros([n_samples0,1]) , np.ones([n_samples1,1])) , axis=0)\n",
    "\n",
    "    if scenario == 1: \n",
    "        # Separate Gaussian\n",
    "        mean0 = [2, 3]\n",
    "        mean1 = [12, 14]\n",
    "        cov0 = [[1, 1.5], [1.5 ,3]]\n",
    "        cov1 = 2 ** 2 * np.eye(2)\n",
    "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
    "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
    "        \n",
    "    elif scenario == 2:\n",
    "        # Overlapping Gaussian\n",
    "        mean0 = [2, 3]\n",
    "        mean1 = [5, 7]\n",
    "        cov0 = [[1, 1.5], [1.5 ,3]]\n",
    "        cov1 = [[2, 3], [3 ,6]]\n",
    "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
    "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
    "        \n",
    "        \n",
    "    elif scenario == 3:\n",
    "        # Overlapping Gaussian\n",
    "        mean0 = [0, 0]\n",
    "        mean1 = [0, 0]\n",
    "        cov0 = [[50, 4], [4, 2]]\n",
    "        cov1 = [[2, 0], [0 ,50]]\n",
    "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
    "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
    "        \n",
    "        \n",
    "    elif scenario == 4:\n",
    "        # Circles\n",
    "        # 1 circle\n",
    "        angle0=np.linspace(0, 2 * np.pi, n_samples0);\n",
    "        X0=np.vstack((8*np.cos(angle0) , 8*np.sin(angle0))).T\n",
    "        \n",
    "        # 2 circle\n",
    "        angle1=np.linspace(0, 2 * np.pi, n_samples1);\n",
    "        X1=np.vstack((2*np.cos(angle1) , 2*np.sin(angle1))).T\n",
    "\n",
    "    return X0,X1,y\n",
    "\n",
    "def plotResults(X=None,U=None,Y=None,const=1,title=''):\n",
    "    \n",
    "    N0=np.sum(y==0)\n",
    "    N1=np.sum(y==1)\n",
    "    \n",
    "    fig=plt.figure(figsize=(17, 6))\n",
    "    \n",
    "    ax  = fig.add_subplot(1, 3, 1)\n",
    "    plt.scatter(X0[:,0],X0[:,1],c='r', label='Class 0')\n",
    "    plt.scatter(X1[:,0],X1[:,1],c='b', label='Class 1')\n",
    "    if U is not None:\n",
    "        average=X.mean(axis=0)\n",
    "        sd=LA.norm(X.std(axis=0))\n",
    "        u0=U[:,0]*const*sd;\n",
    "        u1=U[:,1]*const*sd;\n",
    "        plt.plot([average[0]-u0[0], average[0]+u0[0]],[average[1]-u0[1], average[1]+u0[1]], c='g',linewidth=4, label='C 1' )\n",
    "        plt.plot([average[0]-u1[0], average[0]+u1[0]],[average[1]-u1[1], average[1]+u1[1]], c='k',linewidth=4, label='C 2' )\n",
    "        plt.title('Original data and components')\n",
    "    else:\n",
    "        plt.title('Original data')\n",
    "    plt.legend()\n",
    "    \n",
    "    ax  = fig.add_subplot(1, 3, 2)\n",
    "    plt.scatter(Y[np.where(y == 0)[0],0], np.zeros((N0,1)), c='r', s=3, marker='o', label='Class 0')\n",
    "    plt.scatter(Y[np.where(y == 1)[0],0], np.zeros((N1,1)), c='b', s=3, marker='x', label='Class 1')\n",
    "    ax.set_title(title + '\\n Scores on 1st component')\n",
    "    \n",
    "    ax  = fig.add_subplot(1, 3, 3)\n",
    "    plt.scatter(Y[np.where(y == 0)[0],1], np.zeros((N0,1)), c='r', s=3, marker='o', label='Class 0')\n",
    "    plt.scatter(Y[np.where(y == 1)[0],1], np.zeros((N1,1)), c='b', s=3, marker='x', label='Class 1')\n",
    "    plt.legend()\n",
    "    plt.title('Scores on 2nd component')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def frontiere(model, X, y, step=50):\n",
    "\n",
    "    labels = np.unique(y)\n",
    " \n",
    "    min_tot = np.min(X)\n",
    "    max_tot = np.max(X)\n",
    "    delta = (max_tot - min_tot) / step\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot, max_tot, delta),\n",
    "                         np.arange(min_tot, max_tot, delta))\n",
    "    z = np.array( model.predict(np.c_[xx.ravel(), yy.ravel() ]) )\n",
    "    z = z.reshape(xx.shape)\n",
    "   \n",
    "    plt.imshow(z, origin='lower', extent=[min_tot, max_tot, min_tot, max_tot],\n",
    "               interpolation=\"mitchell\", cmap='RdBu')\n",
    "    \n",
    "    cbar = plt.colorbar(ticks=labels)\n",
    "    cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "    plt.scatter(X[np.where(yKmeans == 0)[0],0],X[np.where(yKmeans == 0)[0],1],c='r', label='Predicted class 0')\n",
    "    plt.scatter(X[np.where(yKmeans == 1)[0],0],X[np.where(yKmeans == 1)[0],1],c='b', label='Predicted class 1') \n",
    "    \n",
    "    plt.ylim([min_tot, max_tot])\n",
    "    plt.xlim([min_tot, max_tot])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,let's create the data we will use.\n",
    "Try the 4 different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0,X1,y = generate_scenario(scenario=1, n_samples0 = 350, n_samples1 = 350)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X0[:,0],X0[:,1],c='r', label='Class 0')\n",
    "plt.scatter(X1[:,0],X1[:,1],c='b', label='Class 1')\n",
    "plt.title('Original data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to use the methods seen this morning. \n",
    "\n",
    "As you can see, we have generated two populations (class 0 and class 1). We concatenate them as a single matrix *X* which will be the input for all methods. In this way, the methods will be unaware of the class of the observations (unsupervised) and we will test whether the methods are appropriate for the analysed scenario and if they are able to use less dimensions to correctly distinguish the two classes. \n",
    "\n",
    "Let's start with PCA. \n",
    "\n",
    "**Question:** Use PCA with the different 4 scenarios and comment the results. When does PCA work well ? How can you undesrtand when it 'works well' ? What does it mean in your opinion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.concatenate((X0,X1),axis=0)\n",
    "\n",
    "## PCA\n",
    "pca = PCA(random_state=1)\n",
    "Ypca=pca.fit_transform(X)\n",
    "U=pca.components_.T # we want PC on columns\n",
    "print(pca.explained_variance_ratio_)\n",
    "plotResults(X,U,Ypca,const=1,title='PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead than using the scikit-learn implementation, implement one on your own !\n",
    "Complete the code where you see **XXXXXXXXXXXXXX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaLecture(X):\n",
    "    ''' \n",
    "    Inputs: \n",
    "            X: is a [Nxd] matrix. Every row is an observation and every\n",
    "              column consists of features.\n",
    "    Outputs:\n",
    "            Y: is a [Nxd] matrix representing the scores, namely the \n",
    "            coordinates of X onto the new basis given by the eigenvactors U\n",
    "            of the covariance matrix of X. Columns are the principal components.\n",
    "               \n",
    "            U: columns are Eigenvectors (sorted from the greatest to the lowest eigenvalue)\n",
    "    \n",
    "            D: Eigenvalues (sorted from the greatest to the lowest eigenvalue)\n",
    "               \n",
    "            var_explained: percentage of the original variability explained\n",
    "            by each principal component.\n",
    "    '''\n",
    "    \n",
    "    N=X.shape[0]\n",
    "    Xc=XXXXXXXXXXXXXX # centering\n",
    "    D2, Uh = LA.svd(Xc)[1:3] # computation of eigenvectors and eigenvalues using SVD\n",
    "    U=Uh.T\n",
    "    Y=XXXXXXXXXXXXXX # computation of the scores\n",
    "    D=D2**2/(N-1) # computation of the eigenvalues\n",
    "    var_explained = XXXXXXXXXXXXXX # computation of explained variance\n",
    "    return Y,U,D,var_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YpcaLec,UpcaLec,DpcaLec,var_explainedPcaLec=pcaLecture(X)\n",
    "plotResults(X,UpcaLec,YpcaLec,const=1,title='PCA Lecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Kernel-PCA with the rbf kernel (you can also test other kernels if you want).\n",
    "\n",
    "**Question:** Use Kernel-PCA with the different 4 scenarios and comment the results. When does K-PCA work well ? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel-PCA\n",
    "gamma=2\n",
    "Kpca = KernelPCA(kernel='rbf', gamma=gamma, random_state=1)\n",
    "YKpca=Kpca.fit_transform(X)\n",
    "DKpca=Kpca.lambdas_\n",
    "AKpca=Kpca.alphas_\n",
    "\n",
    "plotResults(X=X,Y=YKpca,const=1,title='KPCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead than using the scikit-learn implementation, implement one on your own !\n",
    "Complete the code where you see **XXXXXXXXXXXXXX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KpcaGaussianLecture(X,gamma):\n",
    "    '''  \n",
    "    Inputs: \n",
    "            X: is a [Nxd] matrix. Every row is an observation and every\n",
    "            column is a feature.\n",
    " \n",
    "    Outputs:\n",
    "            Y: is a [Nxd] matrix representing the scores, namely the \n",
    "            coordinates of \\phi(X) onto the new basis given by the eigenvactors \n",
    "            of the covariance matrix of \\phi(X). Columns are the principal components.\n",
    "     \n",
    "            D: Eigenvalues (sorted from the greatest to the lowest eigenvalue)\n",
    "\n",
    "    '''\n",
    "       \n",
    "    N=X.shape[0]\n",
    "\n",
    "    # Computation of the Kernel matrix K [N,N]\n",
    "    # Use a rbf kernel\n",
    "    XXXXXXXXXXXXXX  \n",
    "    K=XXXXXXXXXXXXXX\n",
    "    \n",
    "    # center kernel matrix\n",
    "    Kc=XXXXXXXXXXXXXX \n",
    "\n",
    "    # eigenvalue analysis\n",
    "    D,A=LA.eigh(Kc);     \n",
    "    idx = D.argsort()[::-1]  # reverse order to make 'descend' \n",
    "    D = np.real(D[idx])\n",
    "    D[D<0]=1e-18 # make negative eigenvalues positive (and almost 0)\n",
    "    A = np.real(A[:,idx])\n",
    "\n",
    "    # Normalisation eigenvectors\n",
    "    # Norm of every eigenvector is 1, we want it to be 1/sqrt(N*eig)\n",
    "    \n",
    "    An=np.copy(A)\n",
    "    for i in range(N):      \n",
    "        An[:,i]=XXXXXXXXXXXXXX          \n",
    "           \n",
    "    Y=XXXXXXXXXXXXXX  # computation of the scores  \n",
    "    \n",
    "    return Y, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YKpcaLec, DKpcaLec = KpcaGaussianLecture(X,gamma)\n",
    "plotResults(X=X,Y=YKpcaLec,const=1,title='KPCA Lecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test ICA.\n",
    "\n",
    "**Question:** Use ICA with the different 4 scenarios and comment the results. When it works better than PCA ? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ICA\n",
    "ICA= FastICA(whiten=True, fun='exp', max_iter=20000, tol=0.00001, random_state=1)\n",
    "Yica=ICA.fit_transform(X)\n",
    "Aica=ICA.mixing_\n",
    "plotResults(X=X,U=Aica,Y=Yica,const=0.01,title='ICA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead than using the scikit-learn implementation, implement one on your own !\n",
    "Complete the code where you see **XXXXXXXXXXXXXX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FastICALecture(X,N_Iter=3000,tol=1e-5,plot_evolution=0):\n",
    "    '''\n",
    "    Inputs: \n",
    "                   X: is a [d x N] matrix. Every column is an observation \n",
    "                   and every row is a feature.       \n",
    "    \n",
    "                   (Optional) N_Iter: maximum number of iterations\n",
    "    \n",
    "                   (Optional) delta: convergence criteria threshold\n",
    "    \n",
    "                   (Optional) plot_evolution: plot evolution of error\n",
    "                   \n",
    "     Outputs:      \n",
    "                   S: [d x N] matrix.  Each column is an independent component \n",
    "                   of the centred and whitened input data X              \n",
    "                   \n",
    "                   W: [d x d] matrix. It is the demixing matrix. S = W*Xcw \n",
    "    \n",
    "    '''\n",
    "    # Choose G among the Negentropy functions seen during the lecture\n",
    "    # First derivative of G   \n",
    "    def g(t):\n",
    "        res = XXXXXXXXXXXXXX\n",
    "        return res\n",
    "    \n",
    "    # Second derivative of G  \n",
    "    def gp(t):\n",
    "        res = XXXXXXXXXXXXXX\n",
    "        return res\n",
    "    \n",
    "    # Size of X\n",
    "    d,N=X.shape \n",
    "        \n",
    "    # Center data\n",
    "    Xc=XXXXXXXXXXXXXX\n",
    "    \n",
    "    # Whiten data\n",
    "    Xcw=XXXXXXXXXXXXXX\n",
    "    \n",
    "    # check if are whitened\n",
    "    if np.sum(np.eye(d) - np.abs(np.cov(Xcw)))>1e-10:\n",
    "        raise NameError('Your whitening transformation does not work...')\n",
    "    \n",
    "    # Initialize W\n",
    "    W = ortho_group.rvs(d) # random orthogonal matrix \n",
    "    \n",
    "    # delta evolution\n",
    "    k = 0\n",
    "    delta = np.inf # 1 - min(...)\n",
    "    evolutionDelta=[]\n",
    "    \n",
    "    while delta > tol and k < N_Iter:\n",
    "    \n",
    "        k = k + 1\n",
    "        W_old = np.copy(W)\n",
    "        \n",
    "        Wp = XXXXXXXXXXXXXX  \n",
    "        W = XXXXXXXXXXXXXX\n",
    "        if np.sum(np.eye(d)-np.abs(np.dot(W,W.T)))>1e-10:\n",
    "            raise NameError('W should be an orthogonal matrix. Check the computations')\n",
    "                 \n",
    "        delta = XXXXXXXXXXXXXX\n",
    "        evolutionDelta.append(delta)\n",
    "        \n",
    "        if k==1 or k%100==0:\n",
    "            print('Iteration ICA number ', k, ' out of ', N_Iter , ', delta = ', delta)\n",
    "     \n",
    "        \n",
    "    if k==N_Iter:\n",
    "        print('Maximum number of iterations reached ! delta = ', delta)\n",
    "    else:\n",
    "        print('Convergence achieved ( delta = ', delta, ') in ', k, ' iterations')\n",
    "\n",
    "    # Independent components\n",
    "    S = XXXXXXXXXXXXXX\n",
    "            \n",
    "    if plot_evolution==1:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(range(k),evolutionDelta,'bx--', linewidth=4, markersize=12)  \n",
    "        plt.title('Evolution of error - ICA')\n",
    "        plt.show()\n",
    "       \n",
    "    return S,W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SicaLec,WicaLec = FastICALecture(X.T,N_Iter=3000,tol=1e-5,plot_evolution=1)\n",
    "plotResults(X=X, U=WicaLec.T, Y=SicaLec.T, const=1, title='ICA Lecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a different perspective, we could also use K-means. As before, we will use it on X and we will check whether it can well separate the two classes. \n",
    "\n",
    "\n",
    "**Question:** Does it work well in all scenarios ? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means\n",
    "kmeans=KMeans(n_clusters=2)\n",
    "yKmeans=kmeans.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(17, 6))\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[np.where(y == 0)[0],0],X[np.where(y == 0)[0],1],c='r', label='Class 0')\n",
    "plt.scatter(X[np.where(y == 1)[0],0],X[np.where(y == 1)[0],1],c='b', label='Class 1')\n",
    "plt.title('Original data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[np.where(yKmeans == 0)[0],0],X[np.where(yKmeans == 0)[0],1],c='r', label='Predicted class 0')\n",
    "plt.scatter(X[np.where(yKmeans == 1)[0],0],X[np.where(yKmeans == 1)[0],1],c='b', label='Predicted class 1')\n",
    "plt.title('K-Means')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "frontiere(kmeans, X, y, step=50)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

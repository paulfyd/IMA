{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKe0P2J2V6fn"
   },
   "source": [
    "## Face recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxoYGZl-VoSK"
   },
   "source": [
    "Load the original images present in the files *'YaleB\\_32x32.mat'*. This is a small part of the freely available Extended Yale Face Database B downloaded from http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html. It contains 2414 cropped images resized to 32x32 pixels. Every image is represented as a vector 1x1024 and all images are stacked in a matrix called data. There are 38 subjects with around 64 near frontal images per individual under different illumination conditions. Once loaded and normalised the data, such that the pixels are between 0 and 1, you can plot some images using the function *'imshow'*.\n",
    "\n",
    "# Goal\n",
    "The goal of this part is to evaluate the performance of the dimensionality reduction techniques presented this morning for face recognition. We divide the data-set into two parts, training and test. For every dimensionality reduction technique, you will first extract a set of basis images from your training data-set. Then, you will project the test subjects in this new basis and use the nearest neighbor algorithm to evaluate the performance of the dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "al-VlsavWVw6"
   },
   "source": [
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rhu_TEsRA5BW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google_drive_downloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3a682f553a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle_drive_downloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleDriveDownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m gdd.download_file_from_google_drive(file_id='1rgICXtcIAgDqSoHnNXNZMD_iNABF3RZA',\n\u001b[1;32m      3\u001b[0m dest_path='./YaleB_32x32.mat')\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google_drive_downloader'"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "gdd.download_file_from_google_drive(file_id='1rgICXtcIAgDqSoHnNXNZMD_iNABF3RZA',\n",
    "dest_path='./YaleB_32x32.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uY_EMa-KWXrt"
   },
   "source": [
    "Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiSmmYCsBP-V"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from scipy import linalg as LA\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qG3yNVvKWfYw"
   },
   "source": [
    "This is a useful function to plot the basis images. Be careful, each row of data is a basis image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xe_ty98JBejg"
   },
   "outputs": [],
   "source": [
    "def plotFaces(data,r,c,ncol=2,N=0,indeces=None,title=None):\n",
    "    # data: each face is a row in data\n",
    "    # r,c = number of rows and columns of each image\n",
    "    # n_col = number of columns for subplots\n",
    "    # N = random images to plot (used only if indeces is empty)\n",
    "    # indeces = indeces of images to plot\n",
    "    # title = title of the plot\n",
    "\n",
    "   \n",
    "    if indeces is None:\n",
    "        if N==0:\n",
    "            raise NameError('You should define either N or indeces')\n",
    "        else:\n",
    "            print('Use N random subjects')\n",
    "            indeces=np.random.randint(0,data.shape[0],(N,1))\n",
    "            \n",
    "    nrow=math.ceil(len(indeces)/ncol)\n",
    "    \n",
    "    fig=plt.figure(figsize=(17, 6))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, index in enumerate(indeces):\n",
    "        fig.add_subplot(nrow, ncol, i+1)\n",
    "        plt.imshow(np.resize(data[index,:],(r,c)).T,origin='upper',cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    #plt.subplots_adjust(left=0.01, bottom=0.05, right=0.99, top=0.93, wspace=0.04, hspace=0.0)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvJ5nT2VWqCF"
   },
   "source": [
    "Let's load the data and compute some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVfQFQLFBV0w"
   },
   "outputs": [],
   "source": [
    "x = loadmat('./YaleB_32x32.mat')\n",
    "data=x['fea']\n",
    "d=data.shape[1] # number of pixels of the images\n",
    "subjectIndex=x['gnd']\n",
    "maxValue = np.max(np.max(data)) # max intensity value\n",
    "data = data/maxValue; # Scale pixels to [0,1]\n",
    "\n",
    "Ns=len(np.unique(subjectIndex)); # Number subjects\n",
    "Is=round(len(subjectIndex)/Ns) # Number images per subject (on average, not the same number for every subject)\n",
    "r=int(np.sqrt(d)) # number rows of each image\n",
    "c=r # number columns of each image, equal to row since images are square\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtMFoLlsWzrz"
   },
   "source": [
    "Let's plot first 10 images of different subjects and then 10 images of the same subject but with different positions and illumination conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "colab_type": "code",
    "id": "BS22jyD6B2lr",
    "outputId": "9b62e2d3-42f6-4cc2-a25b-7a21c83e2d83"
   },
   "outputs": [],
   "source": [
    "# Plot data      \n",
    "indexDifferent=np.arange(1,Is*40,Is)     \n",
    "plotFaces(data,r,c,ncol=3,indeces=indexDifferent[0:10],title='Different subjects')       \n",
    "indexSame=np.arange(0,10,1)      \n",
    "plotFaces(data,r,c,ncol=2,indeces=indexSame,title='Different positions of the same subjects')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuJXeoPiBw1X"
   },
   "outputs": [],
   "source": [
    "# Create train/test split  \n",
    "trainIdx, testIdx = train_test_split(np.arange(0,data.shape[0],1),test_size=0.3,random_state=1)\n",
    "Xtest=data[testIdx,:]\n",
    "Xctest=Xtest-np.mean(Xtest,axis=0) # centering\n",
    "Xtrain=data[trainIdx,:]\n",
    "Xctrain=Xtrain-np.mean(Xtrain,axis=0) # centering\n",
    "Id_Train = subjectIndex[trainIdx]; # indeces of the subjects for the training\n",
    "Id_Test = subjectIndex[testIdx]; # indeces of the subjects for the test\n",
    "\n",
    "N = Xctrain.shape[0] # N number of training images\n",
    "M = Xctest.shape[0] # M number of test images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iQ-llAPfL_y"
   },
   "source": [
    "As first idea, we could simply use the pixel intensities as features. This is basically like using the original data, without dimensionality reducton techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "NnVJP6T0CUeW",
    "outputId": "e1368d6b-ec4c-4cce-fe5c-766f03e8d18d"
   },
   "outputs": [],
   "source": [
    "## Use the pixel intensities to find the correct subject for the test images\n",
    "NN=KNeighborsClassifier(n_neighbors=1)\n",
    "NN.fit(Xctrain,Id_Train.ravel())\n",
    "print('Percentage of correct answer using the pixel intensities is ', NN.score(Xctest,Id_Test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmYtcEy2XwQ5"
   },
   "source": [
    "# PCA\n",
    "\n",
    "You will first use PCA. Compute the scores, eigenvectors and eigenvalues of the training set. The eigenvectors represent the basis images and they are usually called *'Eigenfaces'*. Then, project both training and test data onto the eigenvectors that explain 95$\\%$ of the variability of the training set. You will obtain two vectors of scores which you will use for evaluating the performance of the algorithm. Use the function *'KNeighborsClassifier'* to test the performance.\n",
    "\n",
    "**Question:** \n",
    "\n",
    "1. Use either the scikit-learn implementation or yours (better!) to compute the PCA for the training data-set. Comment the results.\n",
    "2. Is it worth it in your opinion to compute PCA ? Why ? Hint: think about the performance in your test set and generalizability, so the number of features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "267H-LajClYe"
   },
   "outputs": [],
   "source": [
    "## PCA\n",
    "print('PCA')\n",
    "XXXXXXXXXX\n",
    "\n",
    "# Threshold defined as 99% of the variability\n",
    "Threshold_PCA = 99\n",
    "CumulativePca=np.cumsum(var_explained_pca)\n",
    "indexPCA=np.argwhere(CumulativePca>Threshold_PCA)\n",
    "PCAComp=indexPCA[0][0]\n",
    "\n",
    "# Selection of the eigenvectors \n",
    "Yr_train_PCA=YpcaTrain[:,:PCAComp]\n",
    "Ur_train_PCA=UpcaTrain[:,:PCAComp]\n",
    "\n",
    "# Computation of the test scores using the eigenvectors computed with the\n",
    "# training data-set\n",
    "Yr_test_PCA=np.dot(Xctest,Ur_train_PCA)\n",
    "\n",
    "# Plot the Eigenfaces\n",
    "plotFaces(UpcaTrain.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='PCA - Eigenfaces')       \n",
    "\n",
    "# Score\n",
    "NN.fit(Yr_train_PCA,Id_Train.ravel())\n",
    "print('Percentage of correct answer using PCA is ', NN.score(Yr_test_PCA,Id_Test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkgegzdsdFsm"
   },
   "source": [
    "# ICA\n",
    "\n",
    "In the next section tou will evaluate ICA. Every image $x_i$ can be seen as a linear combination of basis images: $x_i=\\sum_j c_{ij} b_j$ where $c_{ij}$ is a coefficient and $b_j$ is basis image. ICA can be used in two different ways for face recognition. We can look for a set of statistically independent basis images $b_j$ (first architecture) or for a set of statistically independent coefficients $c_{ij}$ (second architecture).\n",
    "\n",
    "In the first architecture, we compute $X'=A'S'$, where every row of $X'$ is an image and the columns are pixels. Images are considered as observations and we look for a set of statistically independent basis images contained in the rows of $S'$.\n",
    "\n",
    "In the second architecture, we transpose the previous setting computing $X''=A''S''$, where every column of $X''$ is an image and rows are pixels. In this case, we consider the pixels as observations and we look for a set of statistically independent coefficients contained in the rows of $S$ and a set of basis images in the columns of $A$.\n",
    "\n",
    "Instead than using the original training data $X$ as input matrix, we are going to use the eigenvectors (first architecture) or the scores (second architecture) computed with PCA, namely $Y=XL$ (same notation as in the slides of the lecture). In this way, we reduce the computational time since the number of eigenvectors that account for 95\\% of the variance of the training images (columns of $L$) is definitely lower than the number of pixels (columns of $X$). If you want, you can of course use the original data but it will take much more time to converge.\n",
    "\n",
    "For the first architecture we will use $L^T$ as input matrix. In fact, we can notice that the PCA approximation of the matrix $X_{train}$, containing an image in every row, can be written as $\\tilde{X}=YL^T$. If we use $L^T$ as input in the ICA algorithm we obtain $L^T=AS$, thus it follows that $\\tilde{X}=YW^TS$ (since $A=W^T$). The basis images are contained in the rows of $S$ and the coefficients used for evaluating the performance are instead contained in the rows of $Y_{train}W^T$ for the training set and in $Y_{test}W^T$ for the test set.\n",
    "\n",
    "For the second architecture, we will instead use $Y^T$ as input matrix. Remember that in the second architecture we want to apply the ICA algorithm to the transpose of $X_{train}$, namely $X^T=AS$. We can notice that, given the PCA transformation $Y=XL$, one can write $AS=Y^T=L^TX^T$ which entails $X^T=LW^TS$. The columns of $LW^T$ contain the basis images whereas the columns of $S$ contain the statistically independent coefficients used to test the performance of the algorithm. For the test set we compute $S_{test}=W_{train}Y_{test}^T$.\n",
    "\n",
    "**Question:**\n",
    " \n",
    "\n",
    "1.   Implement the two architectures and test their performance. Which one is better ?\n",
    "2.   Looking at the basis images, in which case do they seem more 'real' ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxt0bRd8QLZr"
   },
   "outputs": [],
   "source": [
    "# First architecture\n",
    "XXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiUbGQnYUyWG"
   },
   "outputs": [],
   "source": [
    "# Second architecture\n",
    "XXXXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbSo66G1f5Ew"
   },
   "source": [
    "# NNMF\n",
    "\n",
    "Here you will test Non-negative Matrix factorization. The basis images of the training are in the matrix $W_{train}$ and the scores (or coefficients) to test the performance in $H_{train}$. The test scores are computed as $H_{test}=W_{train}X_{test}$.\n",
    "\n",
    "**Question**\n",
    "\n",
    "\n",
    "1.   Implement your own implementation following the lecture slides (62,63,64)\n",
    "2.   Plot the basis images and compare them with respect to the basis images obtained using PCA and ICA\n",
    "3.   Test the performance of NNMF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0F4jrirg4Qq"
   },
   "outputs": [],
   "source": [
    "def NNMFLecture(X,r=X.shape[1],N_Iter=1000,tolerance=1e-5,plot_evolution=1)\n",
    "'''\n",
    "Inputs: \n",
    "%           X: is a [dxN] matrix. Every column (x) is an observation and every\n",
    "%           row consists of features.\n",
    "%\n",
    "%           r: size of the matrices W and H\n",
    "%\n",
    "%           (Optional) N_Iter: maximum number of iterations\n",
    "%\n",
    "%           (Optional) tolerance: convergence criteria threshold\n",
    "%\n",
    "%           (Optional) plot_evolution: plot evolution convergence criteria\n",
    "%\n",
    "% Outputs:\n",
    "%           W: is a [d x r] matrix containing the basis images in its\n",
    "%           columns\n",
    "%           \n",
    "%           H: is a [r x N] matrix containing the loadings (h) in its columns\n",
    "%           of the linear combination: x=Wh \n",
    "%\n",
    "'''\n",
    "XXXXXXXXXX\n",
    "\n",
    "return W,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KHXEwNMkA2N"
   },
   "outputs": [],
   "source": [
    "# NNMF\n",
    "XXXXXXXXX"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP-Unsupervised-1-FaceRecognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
